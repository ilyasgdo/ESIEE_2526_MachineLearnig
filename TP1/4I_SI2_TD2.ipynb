{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD - Recherche de minimum et descente de gradient\n",
    "Dans ce TD, nous nous interesons aux méthodes pour rechercher le minimum d'une fonction. nous allons étudier deux méthodes :\n",
    "- [**Méthode du nombre d'or**](https://fr.wikipedia.org/wiki/M%C3%A9thode_du_nombre_d%27or) (ou [`Golden-section search`](https://en.wikipedia.org/wiki/Golden-section_search) en anglais) pour rechercher le minimum d'une fonction mono-variable\n",
    "- [**Algorithme du gradient**](https://fr.wikipedia.org/wiki/Algorithme_du_gradient) (ou [`Gradient descent`](https://en.wikipedia.org/wiki/Gradient_descent) en anglais) pour rechercher le minimum d'une fonction mutli-variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Jupyter-notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "# VSCode et Google Colab\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient(f, x_opt, x_hist):\n",
    "    \"\"\"\n",
    "    Trace en 2D la fonction f (de R dans R) et dessine l'évolution de x au cours des itérations de la décsente de gradiant\n",
    "    \n",
    "    Parameters\n",
    "        ----------\n",
    "        f : callable\n",
    "            Fonction optimisé (de R dans R)\n",
    "        x_opt : float\n",
    "            valeur de x pour laquelle la fonction f est minimul\n",
    "        x_hist : list de float\n",
    "            Liste de l'historique des valeurs de x obtenu lors de l'optimisation\n",
    "    \"\"\"\n",
    "    x_hist = np.array(x_hist)\n",
    "    x_d = np.max(np.abs(x_hist-x_opt))\n",
    "    x_min = x_opt - x_d*1.1\n",
    "    x_max = x_opt + x_d*1.1\n",
    "    \n",
    "    x = np.arange(x_min, x_max, 0.1)\n",
    "    y = f(x)\n",
    "    plt.figure()\n",
    "    plt.plot(x,y)\n",
    "    f = np.vectorize(f)\n",
    "    plt.plot(x_hist, f(x_hist),marker = 'o', markeredgecolor = 'red', markerfacecolor = 'red');\n",
    "    \n",
    "\n",
    "def plot3d_gradient(f, x_opt, x_hist):\n",
    "    \"\"\"\n",
    "    Trace en 3D la fonction f (de R^2 dans R) et dessine l'évolution de x au cours des itérations de la décsente de gradiant\n",
    "    \n",
    "    Parameters\n",
    "        ----------\n",
    "        f : callable\n",
    "            Fonction optimisé (de R^2 dans R)\n",
    "        x_opt : numpy array (shape = (2, ))\n",
    "            valeur de x pour laquelle la fonction f est minimul\n",
    "        x_hist : list de numpy array (shape = (2, ))\n",
    "            Liste de l'historique des valeurs de x obtenu lors de l'optimisation\n",
    "    \"\"\"\n",
    "    x_hist = np.vstack(x_hist)\n",
    "    delta = np.max(np.abs(x_hist-x_opt))\n",
    "    x_min = x_opt[0] - delta*1.1\n",
    "    x_max = x_opt[0] + delta*1.1\n",
    "    y_min = x_opt[1] - delta*1.1\n",
    "    y_max = x_opt[1] + delta*1.1\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    xs = np.linspace(x_min, x_max, 50)\n",
    "    ys = np.linspace(y_min, y_max, 50)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    \n",
    "    P = np.vstack((X.reshape(-1),Y.reshape(-1))).T\n",
    "    f = np.vectorize(f, signature='(n)->()')\n",
    "    Z = f(P).reshape(X.shape)\n",
    "    \n",
    "    ax.plot_surface(X, Y, Z, rstride=2, cstride=1,linewidth=1, alpha=0.5,antialiased=True)\n",
    "    ax.contour(X, Y, Z)\n",
    "    ax.plot(x_hist[:,0], x_hist[:,1], f(x_hist),marker = 'o', markeredgecolor = 'red', markerfacecolor = 'red')\n",
    "    ax.set_xlabel('X',fontsize=18)\n",
    "    ax.set_ylabel('Y',fontsize=18)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avant toute chose\n",
    "pour ce TD, nous cherchons le minimum de fonction polynomiale d'ordre 2 [convexe](https://fr.wikipedia.org/wiki/Fonction_convexe).\n",
    "\n",
    "### Rappel\n",
    "\n",
    "Une fonction polynomiale d'ordre 2 est de la forme :\n",
    "$$\n",
    "f(x) = a*x^2 + b*x + c\n",
    "$$\n",
    "avec $(a,b,c)\\in \\mathbb{R}^3$.\n",
    "\n",
    "Un polynome d'ordre 2 est convexe quand :\n",
    "$$\n",
    "\\frac{d^2 f}{d x^2}(x) > 0\n",
    "$$\n",
    "**Question** Quelles sont les contraintes sur $a$, $b$ et $c$ pour que f(x) soit convexe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les contraintes sont :\n",
    "- $a$ est ???????\n",
    "- $b$ est ???????\n",
    "- $c$ est ???????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un polynome d'ordre 2 convexe est minimum quand :\n",
    "$$\n",
    "\\frac{d f}{d x}(x) = 0\n",
    "$$\n",
    "**Question** Pour quelle valeur $x_{\\text{opt}}$, $f(x)$ est t-il minimum ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_{\\text{opt}} = ???????\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notre polynome\n",
    "**À faire** Choisissez des valeurs pour $a$, $b$ et $c$, tel-que f(x) soit convexe et que $x_{\\text{min}} \\neq 0$.\n",
    "\n",
    "**Question** Que vaut $x_{\\text{opt}}$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a = ???????\n",
    "$$\n",
    "$$\n",
    "b = ???????\n",
    "$$\n",
    "$$\n",
    "c = ???????\n",
    "$$\n",
    "$$\n",
    "x_{\\text{opt}} = ???????\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire** Complétez la fonction `def compute_f(x)` pour qu'elle calcule le polynome avec vos valeurs de $a$, $b$ et $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f(x):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode du nombre d'or\n",
    "Le méthode du nombre d'or consiste à rechercher le minimum d'une fonction unimodale et mono-variable.\n",
    "Pout utiliser cette méthode, il est nécessaire de savoir que le minimum de la fonction ce situe dans l'intervalle de départ :\n",
    "$$\n",
    "x_1 < x_{\\text{opt}} < x_3\n",
    "$$\n",
    "avec $(x_1, x_3)$ l'intervalle de départ.\n",
    "\n",
    "Comme $f(x)$ est unimodale entre $x_1$ et $x_3$, nous prenons un point $x_2$ compris entre $x_1$ et $x_3$ tel que nous avons $f(x2) < f(x1)$ et $f(x2) < f(x3)$.\n",
    "\n",
    "Nous voulons savoir si le minimum est compris dans l'intervalle $(x_1, x_2)$ ou $(x_2, x_3)$.\n",
    "Pour cela, nous sondons la fonction avec un nouveau point $x_4$.\n",
    "On peux choisir $x_4$ dans l'intervalle $(x_1, x_2)$ ou $(x_2, x_3)$, mais il est plus intéressant de prendre $x_4$ dans l'intervalle le plus grand.\n",
    "Par exemple, nous dirons que l'intervalle le plus grand est $(x_2, x_3)$ donc $x_2 < x_4 < x_3$.\n",
    "\n",
    "Selon la valeur de $f(x_4)$, nous avons 2 cas possibles :\n",
    "- $f(x_4) > f(x_2)$, nous savons alors que le minimum se trouve dans l'intervalle $(x_1, x_4)$\n",
    "- $f(x_4) < f(x_2)$, nous savons alors que le minimum se trouve dans l'intervalle $(x_2, x_3)$\n",
    "\n",
    "Il est alors possible de recommencer sur le nouvel intervalle de manière itérative.\n",
    "\n",
    "La **méthode du nombre d'or** est similaire à la [recherche par dichotomique](https://fr.wikipedia.org/wiki/Recherche_dichotomique), mais le choix des points utilise le nombre d'or plutôt que le nombre 2.\n",
    "\n",
    "Plus precisement, pour l'intervalle $(x_1, x_3)$, alors les sondes $x_2$ et $x_4$ sont :\n",
    "$$\n",
    "x_2 = x_3 - \\frac{x_3-x_1}{\\varphi}\n",
    "$$\n",
    "$$\n",
    "x_4 = x_1 + \\frac{x_3-x_1}{\\varphi}\n",
    "$$\n",
    "\n",
    "avec $\\varphi = \\frac{1+\\sqrt(5)}{2}$\n",
    "\n",
    "Pour déterminer la convergence de la méthode, nous vérifions si :\n",
    "$$\n",
    "|x_2-x_4| < \\epsilon\n",
    "$$\n",
    "avec $\\epsilon$ petit.\n",
    "\n",
    "**À faire** Complétez la fonction `mno(f, x_1, x_3, tol)` qui recherche le minimum de la fonction $f$ entre $x_1$ et $x_3$ avec la méthode du nombre d'or."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mno(f, x_1, x_3, epsilon):\n",
    "    nombre_dor = (math.sqrt(5) + 1) / 2\n",
    "    \n",
    "\n",
    "    return x_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire** Testez la méthode du nombre d'or, sur votre polynome `compute_f(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme du gradient\n",
    "L'algorithme du gradient (ou de descente de gradient) est une algorithme itératif pour trouver le minimum d'une fonction.\n",
    "\n",
    "Pour utiliser l'algorithme, nous devons lui donner uniquement un point de départ $x_0$.\n",
    "Pour trouver le minimum de la fonction, l'algorithme met à jour à chaque itération la position du point $x_t$ en le déplaçant dans la direction du gradient:\n",
    "$$\n",
    "x_{t+1} = x_t - \\mu_t \\frac{d f}{d x} (x_t)\n",
    "$$\n",
    "avec $\\mu_t$ le \"pas\" qui définit de combien nous avançons dans la direction du gradient.\n",
    "\n",
    "Nous avons besoin du gradient de la fonction $f(x)$ dont nous cherchons le minimum.\n",
    "\n",
    "**À faire** Complétez la fonction `def compute_df(x)` pour qu'elle calcule le gradient du polynome avec vos valeurs de $a$, $b$ et $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_df(x):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode de gradient à pas fixe\n",
    "Dans un premier temps, nous considerons le pas fixe : $\\mu_t = \\text{constant}$.\n",
    "\n",
    "Nous voulons étudier l'influence du pas sur la convergence de l'algorithme.\n",
    "\n",
    "**À faire** Complétez la fonction `def algo_gradient1(f,df,mu,x_0,n)` qui calcul `n` itérations de la méthode du gradient en partant de `x_0` et avec un pas fixe de `mu`.\n",
    "La fonction doit retourner une liste contenant l'historique de l'évolution de $x$ au cours des itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_gradient1(f,df,mu,x_0,n):\n",
    "\n",
    "    return x_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire** Choisirez un point de départ et utilisez la méthode du gradient à pas fixe pour différente valeurs du pas (0.0001, 0.1, 0.4, 0.5, 0.6).\n",
    "Utilisez la fonction `plot_gradient` (defini en haut du notebook) pour afficher les différents cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la méthode du gradient, le nombre d'itérations est inconnu à l'avance. Généralement, on poursuit les itérations jusqu'à atteindre le critère d'arrêt suivant :\n",
    "$$\n",
    "\\|\\nabla f(x_k)\\| \\leqslant \\varepsilon\n",
    "$$\n",
    "avec $\\nabla f(x_k)$ le gradient de la fonction $f(x)$.\n",
    "dans le cas mono-variable s'ecrit:\n",
    "$$\n",
    "\\left| \\frac{d f}{d x}(x_k)\\right| \\leqslant \\varepsilon\n",
    "$$\n",
    "**À faire** Complétez la fonction `algo_gradient2(f,df,mu,x_0,espilon)` qui recherche le minimum de la fonction $f$ avec la méthode du gradient (n'oubliez pas le cas où l'algorithme diverge).\n",
    "La fonction doit retourner la valeur de x à l'optimum et une liste contenant l'historique de l'évolution de $x$ au cours des itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_gradient2(f,df,mu,x_0,espilon):\n",
    "    \n",
    "    return x, x_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire** Testez la méthode du gradient, sur votre polynome compute_f(x) et utilisez la fonction `plot_gradient` (defini en haut du notebook) pour afficher l'évolution de votre déscente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas multi-variable\n",
    "\n",
    "Dans le cas multi-variable, \n",
    "$$\n",
    "f\\colon \\mathbb {R}^D  \\to \\mathbb {R} \\\\\n",
    "$$\n",
    "la méthode du gradient est exactement la même que pour le cas mono-variable. La mise à jours s'effectue de la manière suivante :\n",
    "$$\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\mu \\times \\nabla f(\\mathbf{x}_k)\n",
    "$$\n",
    "avec $\\mathbf{x}_k\\in\\mathbb{R}^D$ un vecteur de taille D.\n",
    "\n",
    "Dans la suite, nous utiliserons la fonction quadratique multi-variable suivante :\n",
    "$$\n",
    "\\begin{aligned}f\\colon \\mathbb {R}^D & \\to \\mathbb {R} \\\\\\mathbf{x} & \\mapsto \\mathbf{x}^\\top\\mathbf{A}\\mathbf{x}+\\mathbf{x}^\\top\\mathbf{b}+c.\\end{aligned}\n",
    "$$\n",
    "avec \n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "   2 & 1 \\\\\n",
    "   2 & 4 \n",
    "\\end{bmatrix} \\text{, } \\mathbf{b} = \\begin{bmatrix}\n",
    "   1 \\\\\n",
    "   2  \n",
    "\\end{bmatrix} \\text{ et } c = 2\n",
    "$$\n",
    "le gradient de $f(\\mathbf{x})$ s'ecrit :\n",
    "$$\n",
    "\\begin{aligned}\\nabla f \\colon \\mathbb{R}^D & \\to \\mathbb{R}^D \\\\\\mathbf{x} & \\mapsto (\\mathbf{A}+\\mathbf{A}^\\top)\\mathbf{x}+\\mathbf{b}.\\end{aligned}\n",
    "$$\n",
    "\n",
    "Le $\\mathbf{x}_{\\text{opt}}$ est alors:\n",
    "$$\n",
    "\\mathbf{x}_{\\text{opt}} = -\\frac{1}{23} \\begin{bmatrix} 2\\\\ 5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Les fonctions pour calculer la fonction $f(\\mathbf{x})$ et son gradient sont les suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f2(x):\n",
    "    A = np.array(((2,1),(2,4)))\n",
    "    b = np.array((1,2))\n",
    "    c = 2\n",
    "    \n",
    "    return np.dot(np.dot(x.T,A),x) + np.dot(x.T,b) + c\n",
    "\n",
    "def compute_df2(x):\n",
    "    A = np.array(((2,1),(2,4)))\n",
    "    b = np.array((1,2))\n",
    "    c = 2\n",
    "    \n",
    "    return np.dot(A+A.T,x) + b\n",
    "\n",
    "x_opt = np.array((-2./23,-5./23))\n",
    "\n",
    "print('compute_f2(x_opt)=', compute_f2(x_opt))\n",
    "print('compute_df2(x_opt)=', compute_df2(x_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x` est alors un `np.zeros((2,1))`\n",
    "\n",
    "**À faire** Complétez la fonction `algo_gradient3(f,df,mu,x_0,espilon, max_iter)` qui recherche le minimum de la fonction $f$ avec la méthode du gradient pour le cas multi-variable (n'oubliez pas le cas où l'algorithme diverge et ajouté un critère d'arrête sur le nombre maximum d'itération ). \n",
    "La fonction doit retourner la valeur de x à l''optimum et une liste contenant l'historique de l'évolution de $x$ au cours des itération.\n",
    "Pour calculer la norme d'une vecteur, vous pouvez utiliser la fonction [numpy.linalg.norm](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_gradient3(f,df,mu,x_0,espilon, max_iter):\n",
    "\n",
    "    \n",
    "    return x, x_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire** Testez la méthode du gradient, sur votre la fonction `compute_f2(x)` et utilisez la fonction `plot3d_gradient` (defini en haut du notebook) pour afficher l'évolution de votre déscente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array((-5,-5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Que constatez-vous ?\n",
    "\n",
    "### Méthode du gradient à pas optimal\n",
    "La méthode du gradient à pas fixe demande un grand nombre d'itération pour converger et trouver le pas fixe le plus adapté n'est pas trivial.\n",
    "\n",
    "Pour que la méthode converge plus rapidement et que nous n'ayons plus à choisir le pas, nous pouvons choisir à chaque itération le pas optimal pour la direction du gradient calculée à l'itération.\n",
    "\n",
    "En effet, une fois que nous avons la direction dans laquelle nous voulons avancer, dans notre cas le gradient $-\\nabla f(\\mathbf{x}_k)$, la question est alors de combien doit-t'on avancer dans cette direction pour aller le plus bas ?\n",
    "\n",
    "Cela revient à chercher le minimum de la fonction suivante :\n",
    "$$\n",
    "g(\\mu_k) = f\\left(\\mathbf{x}_{k} - \\mu_k \\times \\nabla f(\\mathbf{x}_k)\\right)\n",
    "$$\n",
    "avec $\\mu_k > 0$.\n",
    "Nous voyons que la fonction $g(\\mu_k)$ est une fonction mono-variable, il est alors donc possible d'utiliser la méthode du nombre d'or pour trouver son minimum.\n",
    "\n",
    "**À faire** Complétez la fonction algo_gradient4(f,df,x_0,espilon, max_iter) qui recherche le minimum de la fonction f avec la méthode du gradient pour le cas multi-variable en calculant le pas optimal à chaque itération avec la méthode du nombre d'or (n'oubliez d'ajouté un critère d'arrête sur le nombre maximum d'itération ).\n",
    "La fonction doit retourner la valeur de x à l''optimum et une liste contenant l'historique de l'évolution de $x$ au cours des itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_gradient4(f,df,x_0,espilon, max_iter):\n",
    "    \n",
    "    return x, x_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire** Testez la méthode du gradient à pas optimal, sur votre la fonction `compute_f2(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array((-5,-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Que constatez-vous ?\n",
    "\n",
    "## Remarque\n",
    "Il existe déjà ces méthodes d'optimisation en python ([scipy.optimize](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)). \n",
    "\n",
    "Nous aurions pu faire comme cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "x_0 = np.array((-5,-5))\n",
    "print(optimize.fmin_cg(compute_f2, x_0, fprime=compute_df2))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
