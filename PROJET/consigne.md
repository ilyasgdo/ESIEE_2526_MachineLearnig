Votre rapport (7 à 15 pages) devra présenter :
un mini état de l’art (3 à 10 méthodes),
une analyse de datasets disponibles,
une réflexion sur les métriques de performance,
les questions de vie privée et sécurité,
et une discussion sur le type d’apprentissage (supervisé, non supervisé, semi-/auto-supervisé).


pdflatex -interaction=nonstopmode -halt-on-error main.tex 

pdflatex -interaction=nonstopmode -halt-on-error main.tex 
pdflatex -interaction=nonstopmode -halt-on-error main.tex 

pdflatex -interaction=nonstopmode -halt-on-error main.tex 

pdflatex -interaction=nonstopmode -halt-on-error main.tex 
pdflatex -interaction=nonstopmode -halt-on-error main.tex 
pdflatex -interaction=nonstopmode -halt-on-error main.tex 
pdflatex -interaction=nonstopmode -halt-on-error main.tex 

pdflatex -interaction=nonstopmode -halt-on-error main.tex 
pdflatex -interaction=nonstopmode -halt-on-error main.tex 

pdflatex -interaction=nonstopmode -halt-on-error main.tex 

<div class="gap-md flex h-full flex-col items-start border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.ultralytics.com/blog/exploring-the-best-computer-vision-datasets-in-2025" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="ultralytics.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=ultralytics.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">ultralytics.com</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Exploring the best computer vision datasets in 2025</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Join us as we take a closer look at the best computer vision datasets of 2025. Learn how diverse and high-quality datasets drive smarter Vision AI solutions.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.labellerr.com/blog/cvpr-2025-part-1/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="labellerr.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=labellerr.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Labellerr AI</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">CVPR 2025: Breakthroughs in Object Detection &amp; ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">CVPR 2025 (June 11–15, Music City Center, Nashville &amp; virtual) features top-tier computer vision research: 3D modeling, multimodal AI, embodied agents, AR/VR, deep learning, workshops, demos, art exhibits and robotics innovations.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://docs.ultralytics.com/guides/yolo-performance-metrics/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="docs.ultralytics.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=docs.ultralytics.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">docs.ultralytics.com</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Performance Metrics Deep Dive - Ultralytics YOLO Docs</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Explore essential YOLO11 performance metrics like mAP, IoU, F1 Score, Precision, and Recall. Learn how to calculate and interpret them for model evaluation.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.reddit.com/r/computervision/comments/1jydymw/is_yolo_still_the_stateofart_for_object_detection/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="reddit.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=reddit.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">reddit</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Is YOLO still the state-of-art for Object Detection in 2025?</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Is YOLO still the state-of-art for Object Detection in 2025?</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://docs.ultralytics.com/compare/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="docs.ultralytics.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=docs.ultralytics.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">docs.ultralytics.com</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Choose the Best Object Detection Model for Your Project</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Explore comprehensive comparisons of Ultralytics YOLO models and other popular object detection models. Dive into detailed analyses to help you select the optimal model for your computer vision tasks.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://github.com/muhammadshiraz/YOLO-Real-Time-Object-Detection" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="github.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=github.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">GitHub</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">muhammadshiraz/YOLO-Real-Time-Object-Detection - GitHub</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">YOLO Real Time Object Detection (YOLO) with OpenCV and Python. - muhammadshiraz/YOLO-Real-Time-Object-Detection</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://imerit.net/resources/blog/real-time-object-detection-using-yolo/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="imerit.net favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=imerit.net" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Imerit Technology Services</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Real-Time Object Detection Using YOLO</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">YOLO (You Only Look Once) is a fast and effective deep neural network (DNN) architecture that can identify and locate multiple objects in video, in real time.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://encord.com/blog/yolo-object-detection-guide/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="encord.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=encord.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Encord Blog</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">YOLO Object Detection Explained: A Beginner's Guide</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">What is YOLO Object Detection? YOLO (You Only Look Once) models are real-time object detection systems that identify and classify objects in a</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.hitechbpo.com/blog/top-object-detection-models.php" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="hitechbpo.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=hitechbpo.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Hitech BPO</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">9 Best Object Detection Models of 2025</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Explore the top object detection models of 2025. Compare their USPs, architecture and applications to find the perfect fit for your needs. Click to read more!</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.resemble.ai/state-art-object-detection-models/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="resemble.ai favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=resemble.ai" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Resemble AI</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Top Object Detection Models of 2025</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Resemble AI AI voice generator for emotional text to speech using realistic voice cloning software.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://arxiv.org/abs/2503.13903" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="arxiv.org favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=arxiv.org" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">arxiv</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Title: TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Authors:Qiang Qi, Xiao Wang Abstract:Video object detection has made significant progress in recent years thanks to convolutional neural networks (CNNs) and vision transformers (ViTs). Typically, CNNs excel at capturing local features but struggle to model global representations. Conversely, ViTs are adept at capturing long-range global features but face challenges in representing local feature details. Off-the-shelf video object detection methods solely rely on CNNs or ViTs to conduct feature...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kowal_Understanding_Video_Transformers_via_Universal_Concept_Discovery_CVPR_2024_paper.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="openaccess.thecvf.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=openaccess.thecvf.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">thecvf</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Understanding Video Transformers via Universal Concept ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">tion is encoded in early layers of image transformers [2, 27]. In the mid-layers (Figure 7, middle), we find that, among other things, all the models learn to localize and track in- dividual objects. This result introduces a new angle to the recently developed field of object-centric representation learning [3, 26, 47, 59]: it invites us to explore how special- ized approaches contribute, given that object concepts natu- rally emerge in video transformers. In addition, all models, except for...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.v7labs.com/blog/vision-transformer-guide" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="v7labs.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=v7labs.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">V7</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Vision Transformer: What It Is &amp; How It Works [2024 Guide]</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">A vision transformer (ViT) is a transformer-like model that handles vision processing tasks. Learn how it works and see some examples.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://github.com/roboflow/rf-detr" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="github.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=github.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">GitHub</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">RF-DETR is a real-time object detection and segmentation ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">RF-DETR is a real-time object detection and segmentation model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning. - roboflow/rf-detr</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://arxiv.org/html/2504.13099v1" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="arxiv.org favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=arxiv.org" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">arxiv</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">RF-DETR Object Detection vs YOLOv12</span></div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://arxiv.org/html/2407.19650v1" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="arxiv.org favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=arxiv.org" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">arxiv</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Practical Video Object Detection via Feature Selection and ... - arXiv</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Yuheng Shi^†^ Tong Zhang^†^ Xiaojie Guo (Received: date / Accepted: date) Compared with still image object detection, video object detection (VOD) needs to particularly concern the high across-frame variation in object appearance, and the diverse deterioration in some frames. In principle, the detection in a certain frame of a video can benefit from information in other frames. Thus, how to effectively aggregate features across different frames is key to the target problem. Most of...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_Identity-Consistent_Aggregation_for_Video_Object_Detection_ICCV_2023_paper.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="openaccess.thecvf.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=openaccess.thecvf.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">thecvf</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Identity-Consistent Aggregation for Video Object Detection</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">lected object queries are considered to have the same object identity and are fed into an ICA module to maintain the identity consistency of the video objects. Finally, the ob- ject queries from all frames are fed into the detection head jointly to obtain their predictions in parallel. When evaluated on the ImageNet VID dataset , ClipVID achieves a significant performance improvement in fast-moving objects, which are the type of objects that suffers mostly from the appearance variations in a...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://image-net.org/static_files/files/Imagenet%202017%20VID.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="flex items-center justify-center border-subtlest ring-subtlest divide-subtlest bg-subtler" style="width: 16px; height: 16px;" rtrvr-listeners="framework" rtrvr-framework="react"><div class="font-sans text-base text-quietest selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><svg role="img" class="inline-flex fill-current tabler-icon relative block" width="20" height="20"><use xlink:href="#pplx-icon-world"></use></svg></div></div><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">image-net</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Speed/Accuracy Trade‐offs for Object Detection from Video</span></div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://stackoverflow.com/questions/44718287/where-can-i-find-imagenet-vid-dataset" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="stackoverflow.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=stackoverflow.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Stack Overflow</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Where can i find ImageNet VID dataset?</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">ImageNet Large-Scale Visual Recognition Challenge 2015 (ILSVRC2015) introduced a task called object-detection-from-video(VID) with a new dataset. So I go to the ILSVER2015 website and try to find ...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://arxiv.org/pdf/1809.03327.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="arxiv.org favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=arxiv.org" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">arxiv</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">YouTube-VOS: A Large-Scale Video Object</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Learning long-term spatial-temporal features directly for video object seg- mentation task is, however, largely limited by the scale of existing video object segmentation datasets. For example, the popular benchmark dataset DAVIS has only 90 short video clips, which is barely sufficient to learn a sequence-to- sequence network from scratch like other video analysis tasks. Even if we com- bine all the videos from available datasets [16,21,22,23,24,25], its scale is still far smaller than many...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://arxiv.org/pdf/1809.00461.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="arxiv.org favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=arxiv.org" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">arxiv</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">YouTube-VOS: Sequence-to-Sequence Video</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Our dataset contains 3,252 YouTube video clips and 78 categories in- cluding common objects and human activities4. This is by far the largest video object segmentation dataset to our knowledge and we have released it at https://youtube-vos.org. Based on this dataset, we propose a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. We demonstrate that our method is able to achieve the best results on our YouTube-VOS test set and...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="http://arxiv.org/pdf/1704.00675v2.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="arxiv.org favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=arxiv.org" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">arxiv</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">1</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">The Densely-Annotated VIdeo Segmentation (DAVIS) initiative provided a new dataset with 50 high-definition sequences with all their frames annotated with object masks at pixel-level accuracy, which has allowed the appearance of a new breed of video object segmentation algorithms , , , that pushed the quality of the results significantly, almost getting to the point of saturation of the original dataset (around 80% performance by and ). We will refer to this version of the dataset as DAVIS...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><div class="group relative flex w-full items-stretch cursor-pointer"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="youtube.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=youtube.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">youtube</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Exploring The COCO Dataset</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">In this video, we take a deep dive into the Microsoft Common Objects in Context Dataset (COCO). We show a COCO object detector live, COCO benchmark results, COCO example images, COCO class distribution, and more! Documentation on mAP: https://blog.roboflow.ai/what-is-mean-average-precision-object-detection/ COCO Leaderboard: https://paperswithcode.com/sota/object-detection-on-coco COCO Explorer: https://cocodataset.org/#explore Roboflow Dataset Health Check:...</div></div></div></div></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><div class="group relative flex w-full items-stretch cursor-pointer"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="youtube.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=youtube.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">youtube</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Ultralytics COCO Dataset Overview | Episode 37</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Unlock the potential of the COCO dataset with Ultralytics! 🚀 In Episode 37, we delve deep into the COCO (Common Objects in Context) dataset, a cornerstone for object detection, segmentation, and captioning in computer vision. Join us as we explore how this extensive dataset, featuring 330,000 images and 80 object categories, is used to benchmark models and train powerful AI solutions like YOLOv5 and YOLOv8. 🔍 Key Highlights: 0:00 - Introduction to Ultralytics COCO Dataset 0:33 - Exploring...</div></div></div></div></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://public.roboflow.com/object-detection" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="public.roboflow.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=public.roboflow.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Roboflow</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Object Detection Datasets</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Download free computer vision datasets labeled for object detection.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://arxiv.org/html/2507.12396v1" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="arxiv.org favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=arxiv.org" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">arxiv</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">OD-VIRAT: A Large-Scale Benchmark for Object Detection ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Realistic human surveillance datasets are crucial for training and evaluating computer vision models under real-world conditions, facilitating the development of robust algorithms for human and human-interacting object detection in complex environments. These datasets need to offer diverse and challenging data to enable a comprehensive assessment of model performance and the creation of more reliable surveillance systems for public safety. To this end, we present two visual object detection...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://blog.roboflow.com/object-detection-metrics/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="blog.roboflow.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=blog.roboflow.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Roboflow Blog</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Key Object Detection Metrics for Computer Vision - Roboflow Blog</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Understand what key object detection metrics are. Learn why they're used and how to use them.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://github.com/rafaelpadilla/Object-Detection-Metrics" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="github.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=github.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">GitHub</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Most popular metrics used to evaluate object detection algorithms.</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Most popular metrics used to evaluate object detection algorithms. - rafaelpadilla/Object-Detection-Metrics</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://labelyourdata.com/articles/object-detection-metrics" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="labelyourdata.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=labelyourdata.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Label Your Data</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Object Detection: Key Metrics for Computer Vision ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Curious about ensuring accuracy in computer vision? ▶️ Discover essential tips for evaluating your model performance using different object detection metrics.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://viso.ai/deep-learning/supervised-vs-unsupervised-learning/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="viso.ai favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=viso.ai" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">viso.ai</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Supervised vs Unsupervised Learning for Computer Vision</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Explore supervised vs unsupervised learning in computer vision, key differences, and best applications. Learn when to apply each for optimal outcomes.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.v7labs.com/blog/supervised-vs-unsupervised-learning" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="v7labs.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=v7labs.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">V7</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Supervised vs. Unsupervised Learning [Differences &amp; ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">See how supervised learning differs from unsupervised learning. Explore supervised and unsupervised learning examples.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.cs.cmu.edu/~xiaolonw/papers/unsupervised_video.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="cs.cmu.edu favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=cs.cmu.edu" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">cmu</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Unsupervised Learning of Visual Representations using Videos</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Xiaolong Wang, Abhinav Gupta Robotics Institute, Carnegie Mellon University Abstract Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neu- ral Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of un- labeled videos from the web to learn visual representations. Our key idea is...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.edpb.europa.eu/sites/default/files/files/file1/edpb_guidelines_201903_video_devices.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="edpb.europa.eu favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=edpb.europa.eu" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">europa</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Guidelines 3/2019 on processing of personal data through ...</span></div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://dataprivacymanager.net/video-surveillance-cctv-under-gdpr/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="dataprivacymanager.net favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=dataprivacymanager.net" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Data Privacy Manager</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Video surveillance under the GDPR</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Find out when is video surveillance legal under the GDPR? Is video footage personal data? What are your rights and obligations under the GDPR...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.edpb.europa.eu/sites/default/files/files/file1/edpb_guidelines_201903_video_devices_en_0.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="edpb.europa.eu favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=edpb.europa.eu" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">europa</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Guidelines 3/2019 on processing of personal data through ...</span></div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.mobotix.com/sites/default/files/2023-10/mx_WP_Data_Protection_en_231025.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="mobotix.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=mobotix.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">mobotix</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Data protection information on video security systems (VSS)</span></div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://pdfs.semanticscholar.org/2d33/cb991624a17a012b7702897230f9fe416d50.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="pdfs.semanticscholar.org favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=pdfs.semanticscholar.org" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">semanticscholar</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Deep Learning based Human Detection in Privacy- ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">then quickly building meaningful information from them, which is quite worrisome. These concerns necessitates development of surveillance system which can perform its task using privacy-preserving videos. Under GDPR law (Asghar et al. 2019) it is necessary that operators who monitor surveillance systems or who interact with video data should have limited access to visually identifiable features of persons. The main of contribution of this paper is method to develop privacy preserved video data...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.optex-europe.com/about/blog/lidar-and-gdpr-a-privacy-first-approach-to-securing-critical-infrastructure" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="optex-europe.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=optex-europe.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">LiDAR and GDPR: a privacy-first approach to securing critical infrastructure</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">LiDAR and GDPR: a privacy-first approach to securing…</span></div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.cnil.fr/en/ai-and-gdpr-cnil-publishes-new-recommendations-support-responsible-innovation" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="cnil.fr favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=cnil.fr" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">cnil.fr</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">AI and GDPR: the CNIL publishes new recommendations ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">GDPR enables innovative AI while respecting personal data The AI Action Summit, organized by France from February 6th to 11th, 2025, will host</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="http://arxiv.org/pdf/2108.13141.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="arxiv.org favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=arxiv.org" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">arxiv</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Robust Privacy-Preserving Motion Detection and Object Tracking in Encrypted Streaming Video</span></div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.reddit.com/r/computervision/comments/1jfmcxm/what_are_the_most_useful_and_stateoftheart_models/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="reddit.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=reddit.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">reddit</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">What are the most useful and state-of-the-art models in computer vision (2025)?</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">What are the most useful and state-of-the-art models in computer vision (2025)?</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://encord.com/blog/video-object-tracking-algorithms/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="encord.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=encord.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Encord Blog</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Top 10 Video Object Tracking Algorithms in 2025 - Encord</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Object tracking has become a fundamental part of the computer vision ecosystem. It powers various modern artificial intelligence applications an</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://github.com/huanglianghua/video-detection-benchmark" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="github.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=github.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">GitHub</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">GitHub - huanglianghua/video-detection-benchmark: Video object detection benchmark.</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Video object detection benchmark. Contribute to huanglianghua/video-detection-benchmark development by creating an account on GitHub.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://homepages.inf.ed.ac.uk/rbf/CAVIAR/PAPERS/WECCV.pdf" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="homepages.inf.ed.ac.uk favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=homepages.inf.ed.ac.uk" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">ac</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">New Performance Evaluation Metrics for Object Detection ...</span></div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://hiringnet.com/object-detection-state-of-the-art-models-in-2025" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="hiringnet.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=hiringnet.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">madailab.s3.us-west-2.amazonaws.com</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Object Detection: State-of-the-Art Models in 2025</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Object detection has emerged as one of the most critical and widely applied computer vision tasks in artificial intelligence.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://viso.ai/deep-learning/object-detection/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="viso.ai favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=viso.ai" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">viso.ai</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Mastering Object Detection: AI's Visionary Frontier - Viso Suite</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Explore object detection, a key AI field in computer vision, with insights into deep learning algorithms and applications in surveillance, tracking, and more.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://github.com/xiaobai1217/Awesome-Video-Datasets" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="github.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=github.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">GitHub</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">xiaobai1217/Awesome-Video-Datasets</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Video datasets. Contribute to xiaobai1217/Awesome-Video-Datasets development by creating an account on GitHub.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.datacamp.com/blog/yolo-object-detection-explained" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="datacamp.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=datacamp.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">DataCamp</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">YOLO Object Detection Explained: A Beginner's Guide | DataCamp</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Understand YOLO object detection, its benefits, how it has evolved over the last few years, and some real-life applications.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0893608023000199" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="sciencedirect.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=sciencedirect.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">sciencedirect</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Few-shot human–object interaction video recognition with ...</span></div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.scribd.com/document/851187720/2025-AEJ-Object-detection-in-real-time-video-surveillance-using-attention-based-transformer-YOLOv8-model" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="scribd.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=scribd.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Scribd</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">[2025-AEJ]Object detection in real-time video surveillance using attention based transformer-YOLOv8 model</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">This research presents a novel real-time object detection framework that integrates the YOLOv8 backbone with an attention mechanism and a Transformer-based detection head, enhancing performance in complex environments. The model achieves high precision (96.78%) and recall (96.89%) rates, with a mean average precision of 89.67%, demonstrating its effectiveness in real-time applications like surveillance and industrial automation. The proposed system addresses challenges such as occlusion and...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12233284/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="flex items-center justify-center border-subtlest ring-subtlest divide-subtlest bg-subtler" style="width: 16px; height: 16px;" rtrvr-listeners="framework" rtrvr-framework="react"><div class="font-sans text-base text-quietest selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><svg role="img" class="inline-flex fill-current tabler-icon relative block" width="20" height="20"><use xlink:href="#pplx-icon-world"></use></svg></div></div><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">PLOS One</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Video swin-CLSTM transformer: Enhancing human action ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">As video data volumes soar exponentially, the significance of video content analysis, particularly Human Action Recognition (HAR), has become increasingly prominent in fields such as intelligent surveillance, sports analytics, medical ...</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://github.com/ArpitaSatsangi/Real-time-Object-Detection-with-YOLO-using-OpenCV" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="github.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=github.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">GitHub</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">GitHub - ArpitaSatsangi/Real-time-Object-Detection-with-YOLO-using-OpenCV: Perform object detection on images and videos using the YOLO (You Only Look Once) model with OpenCV.</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Perform object detection on images and videos using the YOLO (You Only Look Once) model with OpenCV. - ArpitaSatsangi/Real-time-Object-Detection-with-YOLO-using-OpenCV</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.viam.com/post/guide-yolo-model-real-time-object-detection-with-examples" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="viam.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=viam.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">viam.com</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">YOLO model for real-time object detection: A full guide</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Discover how YOLO models excel in real-time object detection, from sports tracking to security. This guide covers YOLO's evolution, key features, and examples to help you use its capabilities.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://github.com/bo-miao/awsome-video-object-segmentation" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="github.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=github.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">GitHub</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">GitHub - bo-miao/awsome-video-object-segmentation</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Contribute to bo-miao/awsome-video-object-segmentation development by creating an account on GitHub.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://github.com/NVlabs/FasterViT" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="github.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=github.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">GitHub</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">[ICLR 2024] Official PyTorch implementation of FasterViT - GitHub</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">[ICLR 2024] Official PyTorch implementation of FasterViT: Fast Vision Transformers with Hierarchical Attention - NVlabs/FasterViT</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://cloix-mendesgil.com/en/legal-insights/it-contracts-data-and-compliance/augmented-camera-gdpr-compliance/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="cloix-mendesgil.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=cloix-mendesgil.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Cloix Mendès-Gil</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Augmented Camera Technology and GDPR Compliance</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Guide on GDPR compliance for augmented cameras in public spaces, covering legal requirements and publisher duties.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://hirevire.com/blog/best-gdpr-compliant-video-interview-software-tools" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="hirevire.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=hirevire.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">hirevire.com</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">10 Best GDPR Compliant Video Interview Software Tools for 2025</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Discover the 10 best video interview platform options for 2025, ensuring GDPR compliance and strong data protection for secure virtual hiring.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="aws.amazon.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=aws.amazon.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Amazon Web Services, Inc.</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Supervised vs Unsupervised Learning - Difference ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">What's the Difference Between Supervised and Unsupervised Machine Learning? How to Use Supervised and Unsupervised Machine Learning with AWS.</div></div></div></div></div></a></div></div><div class="gap-sm flex w-full flex-row-reverse"><div class="w-full min-w-0"><a rel="noopener" class="group flex size-full cursor-pointer items-stretch" target="_blank" href="https://www.ibm.com/think/topics/supervised-vs-unsupervised-learning" rtrvr-listeners="click,drag,dragend,dragstart" rtrvr-framework="react"><div class="w-full"><div class="group relative flex w-full items-stretch"><div class="flex w-full rounded-lg border-subtlest ring-subtlest divide-subtlest transition duration-normal bg-subtler md:hover:!bg-subtle"><div class="gap-xs pointer-events-none relative flex size-full max-w-full select-none flex-col p-md"><div class="flex w-full items-center justify-between"><div class="space-x-xs flex"><div class="flex items-center gap-x-1.5 border-subtlest ring-subtlest divide-subtlest bg-transparent"><div class="relative -mt-px flex-none font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><div class="flex size-4 items-center justify-center"><div class="relative shrink-0 overflow-hidden rounded-full" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 bg-white"></div><img class="relative block" alt="ibm.com favicon" width="16" height="16" src="https://www.google.com/s2/favicons?sz=128&amp;domain=ibm.com" style="width: 16px; height: 16px;"><div class="rounded-inherit absolute inset-0 border border-[black]/10 dark:border-transparent"></div></div></div></div><div class="line-clamp-1 min-w-0 grow break-all leading-none transition-all duration-300 font-sans text-xs font-medium text-quiet  selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">Supervised vs. Unsupervised Learning: What’s the Difference?</div></div></div><div class="ml-auto shrink-0"></div></div><div class="font-sans text-base text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super"><span class="line-clamp-1 text-left md:line-clamp-2">Supervised vs. Unsupervised Learning: What's the ...</span></div><div class="mt-two line-clamp-1 font-normal md:line-clamp-4 font-sans text-sm text-foreground selection:bg-super/50 selection:text-foreground dark:selection:bg-super/10 dark:selection:text-super">In this article, we’ll explore the basics of two data science approaches: supervised and unsupervised. Find out which approach is right for your situation. The world is getting “smarter” every day, and to keep up with consumer expectations, companies are increasingly using machine learning algorithms to make things easier.</div></div></div></div></div></a></div></div></div>