9. Conclusion
La reconnaissance d’objets dans les flux vidéo s’appuie aujourd’hui sur un panorama riche de solutions ML, allant des détecteurs unifiés à une passe (YOLO) aux architectures Transformer vidéo, en passant par des approches hybrides combinant supervision faible et auto-supervision. En 2025, YOLOv11 reste la référence pour les applications nécessitant une vitesse extrême, tandis que les Transformers vidéo (TGBFormer, ViViT) offrent une précision et une compréhension temporelle supérieures au prix d’une plus grande complexité. Les méthodes émergentes, comme les Spiking Neural Networks et les stratégies faiblement supervisées (PointSR, DOtA), promettent de réduire drastiquement les coûts d’annotation et d’ouvrir de nouveaux cas d’usage embarqués.

Les datasets disponibles sont désormais diversifiés et volumineux : ImageNet VID et YouTube-VOS couvrent des scènes variées avec annotations boxes et masques, COCO fournit une base d’images statiques dense, DAVIS sert de référence pour la segmentation pixel-level, et OD-VIRAT expose les défis de la surveillance réaliste. Les métriques standardisées (IoU, mAP@0.50:0.95, F1-Score, FPS, latence) garantissent une comparaison rigoureuse entre modèles.

Le cadre RGPD impose un impératif de conformité qui reste entièrement réalisable : choisir une base légale adaptée (intérêt légitime), réaliser une DPIA, appliquer Privacy by Design, et mettre en œuvre chiffrement, anonymisation et procédures d’exercice des droits. La combinaison d’un déploiement phasé, d’une architecture technique robuste et d’un suivi juridique assure la faisabilité du projet.

À moyen et long terme, l’évolution naturelle du domaine s’oriente vers des approches hybrides pour optimiser coût et performance, l’edge computing pour réduire latence et préserver la vie privée, et des techniques de traitement temps réel toujours plus efficaces. Ces perspectives ouvrent la voie à des systèmes de reconnaissance d’objets vidéo à la fois performants, scalables et respectueux des droits fondamentaux.